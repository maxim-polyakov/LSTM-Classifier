{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import defaultdict\n",
    "from collections import  Counter\n",
    "plt.style.use('ggplot')\n",
    "import re\n",
    "import sklearn.metrics \n",
    "from nltk.tokenize import word_tokenize\n",
    "import gensim\n",
    "import string\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding,LSTM,Dense,Dropout\n",
    "from keras.initializers import Constant\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "#modelbin = load_model('my_modelbin.h5')\n",
    "#modelplu = load_model('my_modelplu.h5')\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from pymystem3 import Mystem\n",
    "from string import punctuation\n",
    "import re\n",
    "mystem = Mystem() \n",
    "russian_stopwords = stopwords.words(\"russian\")\n",
    "english_stopwords = stopwords.words(\"english\")\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = mystem.lemmatize(text.lower())\n",
    "    tokens = [token for token in tokens if token not in russian_stopwords\\\n",
    "              and token != \" \" \\\n",
    "              and token.strip() not in punctuation]\n",
    "    tokens = [token for token in tokens if token not in english_stopwords]\n",
    "\n",
    "              \n",
    "\n",
    "    text = \" \".join(tokens)\n",
    "    pattern3= r\"[\\d]\"\n",
    "    pattern2=\"[.]\"\n",
    "    text=re.sub(pattern3, \"\", text)\n",
    "    text=re.sub(pattern2, \"\", text)\n",
    "    if(text!='') and len(text)>3:\n",
    "        return text\n",
    "    else:\n",
    "        return \"#VALID!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.preprocessing import sequence\n",
    "from tensorflow.python.keras.preprocessing import text\n",
    "TOP_K = 20000\n",
    "EMBEDDING_VECTOR_LENGTH = 50 # <=200\n",
    "MAX_SEQUENCE_LENGTH = 50\n",
    "\n",
    "class CustomTokenizer:\n",
    "    def __init__(self, train_texts):\n",
    "        self.train_texts = train_texts\n",
    "        self.tokenizer = Tokenizer(num_words=TOP_K)\n",
    "    def train_tokenize(self):\n",
    "        # Get max sequence length.\n",
    "        max_length = len(max(self.train_texts , key=len))\n",
    "        print(max_length)\n",
    "        self.max_length = min(max_length, MAX_SEQUENCE_LENGTH)\n",
    "    \n",
    "        # Create vocabulary with training texts.\n",
    "        self.tokenizer.fit_on_texts(self.train_texts)\n",
    "        \n",
    "    def vectorize_input(self, tweets):\n",
    "        # Vectorize training and validation texts.\n",
    "        \n",
    "        tweets = self.tokenizer.texts_to_sequences(tweets)\n",
    "        # Fix sequence length to max value. Sequences shorter than the length are\n",
    "        # padded in the beginning and sequences longer are truncated\n",
    "        # at the beginning.\n",
    "        tweets = sequence.pad_sequences(tweets, maxlen=self.max_length, truncating='post',padding='post')\n",
    "        return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelbin = load_model('my_modelbin.h5')\n",
    "modelplu = load_model('my_modelplu.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1395ffc605e24ded914b6bae9d5c4404",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb5f340762e24f3284931e3efcf57a86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='info', description='Классификация', icon='check', style=ButtonStyle(), tooltip='Click me'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2964040946846f08953bb68ee7b1fe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import pickle\n",
    "from tensorflow.python.keras.preprocessing import sequence\n",
    "from tensorflow.python.keras.preprocessing import text\n",
    "text_to_display = widgets.Text()\n",
    "display(text_to_display)\n",
    "text = []\n",
    "def Ppredictbin(inpt):\n",
    "    inp = []\n",
    "    for i in inpt:\n",
    "        inp.append(preprocess_text(i))\n",
    "    print(inp)\n",
    "    inn =[]\n",
    "    inn.append(inp.pop())\n",
    "    #print(inn)\n",
    "    with open('tokenizer.pickle', 'rb') as handle:\n",
    "        tokenizer = pickle.load(handle)\n",
    "    tokenized_inpt = tokenizer.vectorize_input(inn)\n",
    "\n",
    "\n",
    "    #print(tokenized_inpt)\n",
    "    score = modelbin.predict(tokenized_inpt)\n",
    "    outpt = max(np.round(score).astype(int))\n",
    "    outscore = max(score)\n",
    "    print(outpt[0], outscore[0])\n",
    "\n",
    "\n",
    "def Ppredictplu(inpt):\n",
    "    mapa={0:\"Анализ данных\",1:\"Провел встречу\",2:\"Подготовил отчет\",3:\"Разработал функциональность\",4:\"Сделал документацию\",5:\"Развертывание сервера\",6:\"Отпуск\",7:\"Обучение\"}\n",
    "    inp = []\n",
    "    for i in inpt:\n",
    "        inp.append(preprocess_text(i))\n",
    "    #print(inp)\n",
    "    inn =[]\n",
    "    inn.append(inp.pop())\n",
    "    #print(inn)\n",
    "    with open('tokenizer.pickle', 'rb') as handle:\n",
    "        tokenizer = pickle.load(handle)\n",
    "    tokenized_inpt = tokenizer.vectorize_input(inn)\n",
    "    print(tokenized_inpt)\n",
    "    scoreplu = modelplu.predict(tokenized_inpt)\n",
    "    #print(score[0])\n",
    "    maxarr = []\n",
    "    maxarrout = []\n",
    "    maxarval=[]\n",
    "    for i in range(len(scoreplu)):\n",
    "        maxarr.append([np.argmax(scoreplu[i])])\n",
    "        maxarval.append(max(scoreplu[i]))\n",
    "    for i in maxarr:\n",
    "        maxarrout.append(i[0])\n",
    "    outpt = []\n",
    "    outpt.append(mapa[maxarrout[np.argmax(maxarval)]])\n",
    "    print(outpt[0],max(maxarval))\n",
    "def on_button_clicked(b):\n",
    "    with out:\n",
    "        clear_output()\n",
    "        text.append(text_to_display.value)\n",
    "        print(Ppredictbin(text),Ppredictplu(text))\n",
    "\n",
    "b = widgets.Button(\n",
    "    description='Классификация',\n",
    "    disabled=False,\n",
    "    button_style='info',\n",
    "    tooltip='Click me',\n",
    "    icon='check'\n",
    ")\n",
    "display(b)\n",
    "\n",
    "out = widgets.Output()\n",
    "display(out)\n",
    "\n",
    "b.on_click(on_button_clicked)\n",
    "\n",
    "text = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
